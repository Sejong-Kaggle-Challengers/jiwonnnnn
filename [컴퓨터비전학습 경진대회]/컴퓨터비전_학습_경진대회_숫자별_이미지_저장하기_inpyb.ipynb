{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "컴퓨터비전 학습 경진대회_숫자별 이미지 저장하기.inpyb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJPAsyjpXzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19fe4f4d-8934-4ae0-b231-097b240be1a0"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/월간데이터7/data.zip'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/월간데이터7/data.zip\n",
            "replace submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: submission.csv          \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dTJxCv81Md"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "import torch\r\n",
        "from torchvision import datasets, models, transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "from random import *\r\n",
        "import time\r\n",
        "import copy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ue7GCsk9NBr"
      },
      "source": [
        "# 데이터 로드하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZbZgaT19Qm1"
      },
      "source": [
        "train.shape : (2048, 787)\r\n",
        "\r\n",
        "test.shape : (20480, 786)\r\n",
        "\r\n",
        "digit : 0~9(10개)\r\n",
        "\r\n",
        "letter : A~Z(26개)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "I9tVt0_-9LjR",
        "outputId": "863167cc-bbd8-4b14-bd3f-cca975c292ff"
      },
      "source": [
        "train = pd.read_csv('train.csv')\r\n",
        "test = pd.read_csv('test.csv')\r\n",
        "submission = pd.read_csv('submission.csv')\r\n",
        "\r\n",
        "print(train.shape)\r\n",
        "train.head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2048, 787)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "      <th>letter</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>L</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>L</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>A</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>C</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>F</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>J</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 787 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  digit letter  0  1  2  3  4  ...  776  777  778  779  780  781  782  783\n",
              "0   1      5      L  1  1  1  4  3  ...    0    1    2    4    4    4    3    4\n",
              "1   2      0      B  0  4  0  0  4  ...    0    1    4    1    4    2    1    2\n",
              "2   3      4      L  1  1  2  2  1  ...    3    0    2    0    3    0    2    2\n",
              "3   4      9      D  1  2  0  2  0  ...    2    0    1    4    0    0    1    1\n",
              "4   5      6      A  3  0  2  4  0  ...    3    2    1    3    4    3    1    2\n",
              "5   6      8      C  4  3  0  3  3  ...    0    4    4    4    2    2    3    4\n",
              "6   7      1      Q  0  0  4  2  4  ...    2    0    4    4    4    3    1    3\n",
              "7   8      3      M  1  0  3  4  4  ...    4    4    4    0    2    2    3    1\n",
              "8   9      6      F  0  1  0  4  0  ...    4    4    4    1    0    1    3    3\n",
              "9  10      8      J  4  3  4  0  0  ...    0    1    3    0    3    3    1    2\n",
              "\n",
              "[10 rows x 787 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWDgM_n970_"
      },
      "source": [
        "## train, test 숫자별로 폴더에 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLNxHWrd9bkE"
      },
      "source": [
        "def make_folder(directory_path):\r\n",
        "    if not os.path.isdir(directory_path): #경로에 해당하는 디렉토리가 없으면 \r\n",
        "        os.mkdir(directory_path) #폴더 생성할 때 사용하는 함수\r\n",
        "\r\n",
        "#os.getcwd() : 현재 작업 디렉토리 반환 지금은 /content\r\n",
        "path_train = os.path.join(os.getcwd(), 'train') #경로를 병합해서 새 경로 생성하기\r\n",
        "path_val = os.path.join(os.getcwd(), 'val')\r\n",
        "path_test = os.path.join(os.getcwd(), 'test')\r\n",
        "\r\n",
        "make_folder(path_train)\r\n",
        "make_folder(path_val)\r\n",
        "make_folder(path_test)\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    path_train_digit = os.path.join(path_train, str(i)) #0~9까지 경로\r\n",
        "    path_val_digit = os.path.join(path_val, str(i))\r\n",
        "    make_folder(path_train_digit) # train 폴더 안에 숫자별 폴더 만들기\r\n",
        "    make_folder(path_val_digit)\r\n",
        "\r\n",
        "for i in range(len(train)):\r\n",
        "    digit = train.loc[i, 'digit']\r\n",
        "    letter = train.loc[i, 'letter']\r\n",
        "    img = train.loc[i, '0':].values.reshape(28,28).astype(int)\r\n",
        "\r\n",
        "    path_train_digit = os.path.join(path_train, str(digit)) # 현재 이미지의 숫자에 맞는 경로\r\n",
        "    path_val_digit = os.path.join(path_val, str(digit))\r\n",
        "\r\n",
        "    if digit==0:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4) # 0.2만큼 validation set 만들어주려고 설정한 듯?..\r\n",
        "        #randint(시작, n-1) 중 랜덤숫자 1개 뽑아내기\r\n",
        "        if ran_num == 0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else : \r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 1:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "            \r\n",
        "    elif digit == 2:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 3:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 4:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 5:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 6:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 7:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 8:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "    elif digit == 9:\r\n",
        "        path_train_image = os.path.join(path_train_digit, '%d_%c.jpg'%(i,letter))\r\n",
        "        path_val_image = os.path.join(path_val_digit, '%d_%c.jpg'%(i, letter))\r\n",
        "        ran_num = randint(0,4)\r\n",
        "        if ran_num ==0:\r\n",
        "            cv2.imwrite(path_val_image, img)\r\n",
        "        else:\r\n",
        "            cv2.imwrite(path_train_image, img)\r\n",
        "\r\n",
        "for i in range(len(test)):\r\n",
        "    letter = test.loc[i, 'letter']\r\n",
        "    img = test.loc[i, '0':].values.reshape(28,28).astype(int)\r\n",
        "    path_test_digit = os.path.join(path_test, '%d_%c.jpg'%(i, letter))\r\n",
        "    cv2.imwrite(path_test_digit, img)\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "QY5UGn8AYPQQ",
        "outputId": "75bacfba-0258-4782-909f-15e6ec51b5fb"
      },
      "source": [
        "plt.title('index: %i, digit: %s, letter: %s'%(1, digit, letter))\r\n",
        "plt.imshow(img)\r\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BeAb9Fhejcx"
      },
      "source": [
        "## 이미지 augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkDMhQRwax2T"
      },
      "source": [
        "img_size = 64\r\n",
        "data_transforms = {\r\n",
        "    'train' : transforms.Compose([\r\n",
        "        transforms.Resize([img_size, img_size]),\r\n",
        "        transforms.RandomHorizontalFlip(),\r\n",
        "        transforms.RandomRotation(20),\r\n",
        "        transforms.Grayscale(num_output_channels=1),\r\n",
        "        transforms.ToTensor(), #데이터 타입을 tensor로 변경\r\n",
        "        transforms.Normalize([0.5],[0.5])\r\n",
        "        #이미지의 경우 픽셀 값이 0~255의 값을 가지는데, ToTensor로 0~1사이의 값으로 바뀜\r\n",
        "        #normalize시 -1~1사이의 값으로 normalize 됨\r\n",
        "\r\n",
        "        # ToTensor ==> scaling\r\n",
        "        # Normalize ==> centerizing+rescaling\r\n",
        "    ]),\r\n",
        "    'val':transforms.Compose([\r\n",
        "        transforms.Grayscale(num_output_channels=1),\r\n",
        "        transforms.Resize([img_size, img_size]),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize([0.5],[0.5])\r\n",
        "    ])\r\n",
        "}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxAgrh5Tfcs0"
      },
      "source": [
        "image folder 관련 설명 \r\n",
        "\r\n",
        ": https://ndb796.tistory.com/373"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gClteylenlZ",
        "outputId": "58657bb0-7e91-4a72-fe8f-066b9a46782f"
      },
      "source": [
        "data_dir = os.path.join(os.getcwd())\r\n",
        "#ImageFolder라이브러리 == 계층적인 폴더 구조를 가지고 있는 데이터셋을 불러올 때 사용.\r\n",
        "image_datasets = {x:datasets.ImageFolder(os.path.join(data_dir, x),\r\n",
        "                                         data_transforms[x]) for x in ['train', 'val']}\r\n",
        "\r\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20, shuffle=True, num_workers=20) for x in ['train', 'val']}\r\n",
        "dataset_sizes = {x:len(image_datasets[x]) for x in ['train', 'val']}\r\n",
        "class_names = image_datasets['train'].classes\r\n",
        "\r\n",
        "print(dataset_sizes, class_names)\r\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 1974, 'val': 652} ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzgSx2Ztbdv-"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsua8wIwkCaZ",
        "outputId": "94617e12-707a-41b8-edf9-cb3f99a740dc"
      },
      "source": [
        "#GPU\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jz25z-MkLaq"
      },
      "source": [
        "class CustomCNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(CustomCNN, self).__init__()\r\n",
        "\r\n",
        "        self.layer1 = self.conv_module(1, 16)\r\n",
        "        self.layer3 = self.conv_module(16, 32)\r\n",
        "        self.layer4 = self.conv_module(32, 64)\r\n",
        "        self.layer5 = self.conv_module(64, 128)\r\n",
        "        self.layer6 = self.conv_module(128,256)\r\n",
        "        self.gap = self.global_avg_pool(256, 10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.layer1(x)\r\n",
        "        out = self.layer3(out)\r\n",
        "        out = self.layer4(out)\r\n",
        "        out = self.layer5(out)\r\n",
        "        out = self.layer6(out)\r\n",
        "        out = self.gap(out)\r\n",
        "        out = out.view(-1, 10)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "    def conv_module(self, in_num, out_num):\r\n",
        "        return nn.Sequential(\r\n",
        "            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(out_num),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1))\r\n",
        "\r\n",
        "    def global_avg_pool(self, in_num, out_num):\r\n",
        "        return nn.Sequential(\r\n",
        "            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(out_num),\r\n",
        "            nn.LeakyReLU(),\r\n",
        "            nn.AdaptiveAvgPool2d((1, 1)))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MAHUctZkWt3",
        "outputId": "2b44d0b1-8b36-457a-810c-d6dff4bc61a9"
      },
      "source": [
        "model = CustomCNN()\r\n",
        "model = model.to(device)\r\n",
        "print(model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomCNN(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer5): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer6): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (gap): Sequential(\n",
            "    (0): Conv2d(256, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfCWd_6KleYo"
      },
      "source": [
        "learning_scheduler \r\n",
        "\r\n",
        ": https://sanghyu.tistory.com/113"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6pH2WaSkbds"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\r\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.1,patience=5)\r\n",
        "\r\n",
        "#ReducdLROnPlateau (optimizer, factor(감소시킬 비율), patience(metric이 향상 안 될 때, 몇 epoch를 참을것인가?))\r\n",
        "# : 성능이 향상이 없을 때 learning rate를 감소시킨다.  그렇기 때문에 validation loss나 metric을 learning rate step 함수의 input으로 넣어주어야 한다.\r\n",
        "# 그래서 metric이 향상되지 않을 때, patience횟수(epoch)만큼 참고 그 이후에는 learning rate를 줄인다. \r\n",
        "# optimizer에 momentum을 설정해야 사용가능"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRZ4eGxOlQag"
      },
      "source": [
        "\r\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=8):\r\n",
        "    global_info=[]\r\n",
        "    since = time.time()\r\n",
        "\r\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) #deepcopy 내부에 객체들까지 새롭게 copy 되는 것\r\n",
        "    best_acc = 0.0\r\n",
        "    early_stopping = EarlyStopping(patience=11, verbose=True)\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        local_info = []\r\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validation phase\r\n",
        "        for phase in ['train', 'val']:\r\n",
        "            if phase == 'train':\r\n",
        "\r\n",
        "                model.train()  # Set model to training mode\r\n",
        "            else:\r\n",
        "\r\n",
        "                model.eval()   # Set model to evaluate mode\r\n",
        "                if epoch >0:\r\n",
        "                    scheduler.step(val_loss)\r\n",
        "            running_loss = 0.0\r\n",
        "            running_corrects = 0\r\n",
        "\r\n",
        "            # Iterate over data.\r\n",
        "            for inputs, labels in dataloaders[phase]:\r\n",
        "                inputs = inputs.to(device)\r\n",
        "                \r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # zero the parameter gradients\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                # forward\r\n",
        "                # track history if only in train\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "                    outputs = model(inputs)\r\n",
        "                    _, preds = torch.max(outputs, 1)\r\n",
        "                    loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "                    # backward + optimize only if in training phase\r\n",
        "                    if phase == 'train':\r\n",
        "                        loss.backward()\r\n",
        "                        optimizer.step()\r\n",
        "\r\n",
        "                # statistics\r\n",
        "                running_loss += loss.item() * inputs.size(0)\r\n",
        "                running_corrects += torch.sum(preds == labels.data)\r\n",
        "\r\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\r\n",
        "            if phase == 'val':\r\n",
        "                val_loss = running_loss / dataset_sizes['val']\r\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\r\n",
        "            #(Variable(x).data).cpu().numpy()\r\n",
        "            if phase == 'train':\r\n",
        "                local_info.append(epoch_loss)\r\n",
        "                ea = epoch_acc.cpu().numpy()\r\n",
        "                local_info.append(ea)\r\n",
        "            else:\r\n",
        "                local_info.append(epoch_loss)\r\n",
        "                ea = epoch_acc.cpu().numpy()\r\n",
        "                local_info.append(ea)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\r\n",
        "                phase, epoch_loss, epoch_acc))\r\n",
        "\r\n",
        "\r\n",
        "            # deep copy the model\r\n",
        "            if phase == 'val' and epoch_acc > best_acc:\r\n",
        "                best_acc = epoch_acc\r\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "        lr_get = get_lr(optimizer)\r\n",
        "        print(\"Current learning rate : {:.8f}\".format(lr_get))\r\n",
        "        global_info.append(local_info)\r\n",
        "        if phase =='val':\r\n",
        "            early_stopping(epoch_loss, model)\r\n",
        "\r\n",
        "            if early_stopping.early_stop:\r\n",
        "                print(\"Early stopping\")\r\n",
        "                break\r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
        "        time_elapsed // 60, time_elapsed % 60))\r\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\r\n",
        "\r\n",
        "    # load best model weights\r\n",
        "    model.load_state_dict(best_model_wts)\r\n",
        "    return model\r\n",
        "\r\n",
        "def get_lr(optimizer):\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        return param_group['lr']\r\n",
        "    \r\n",
        "class EarlyStopping:\r\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\r\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            patience (int): How long to wait after last time validation loss improved.\r\n",
        "                            Default: 7\r\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \r\n",
        "                            Default: False\r\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\r\n",
        "                            Default: 0\r\n",
        "            path (str): Path for the checkpoint to be saved to.\r\n",
        "                            Default: 'checkpoint.pt'\r\n",
        "            trace_func (function): trace print function.\r\n",
        "                            Default: print            \r\n",
        "        \"\"\"\r\n",
        "        self.patience = patience\r\n",
        "        self.verbose = verbose\r\n",
        "        self.counter = 0\r\n",
        "        self.best_score = None\r\n",
        "        self.early_stop = False\r\n",
        "        self.val_loss_min = np.Inf\r\n",
        "        self.delta = delta\r\n",
        "        self.path = path\r\n",
        "        self.trace_func = trace_func\r\n",
        "    def __call__(self, val_loss, model):\r\n",
        "\r\n",
        "        score = -val_loss\r\n",
        "\r\n",
        "        if self.best_score is None:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(val_loss, model)\r\n",
        "        elif score < self.best_score + self.delta:\r\n",
        "            self.counter += 1\r\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\r\n",
        "            if self.counter >= self.patience:\r\n",
        "                self.early_stop = True\r\n",
        "        else:\r\n",
        "            self.best_score = score\r\n",
        "            self.save_checkpoint(val_loss, model)\r\n",
        "            self.counter = 0\r\n",
        "\r\n",
        "    def save_checkpoint(self, val_loss, model):\r\n",
        "        '''Saves model when validation loss decrease.'''\r\n",
        "        if self.verbose:\r\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\r\n",
        "        torch.save(model.state_dict(), self.path)\r\n",
        "        self.val_loss_min = val_loss\r\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Yk95VEpEXQ",
        "outputId": "419ccad1-011f-417f-c0e8-05b8c7172359"
      },
      "source": [
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\r\n",
        "                       num_epochs=100)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 2.2542 Acc: 0.1788\n",
            "val Loss: 2.2624 Acc: 0.1779\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (inf --> 2.262434).  Saving model ...\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 2.2062 Acc: 0.2330\n",
            "val Loss: 2.1642 Acc: 0.2623\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.262434 --> 2.164191).  Saving model ...\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 2.1862 Acc: 0.2609\n",
            "val Loss: 2.1690 Acc: 0.2331\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 2.1716 Acc: 0.2720\n",
            "val Loss: 2.1487 Acc: 0.2546\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.164191 --> 2.148703).  Saving model ...\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 2.1556 Acc: 0.2741\n",
            "val Loss: 2.1374 Acc: 0.2347\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.148703 --> 2.137373).  Saving model ...\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 2.1420 Acc: 0.2852\n",
            "val Loss: 2.1448 Acc: 0.1871\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 2.1336 Acc: 0.2882\n",
            "val Loss: 2.1410 Acc: 0.2071\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 2.1238 Acc: 0.2842\n",
            "val Loss: 2.1419 Acc: 0.2239\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 2.1166 Acc: 0.2806\n",
            "val Loss: 2.1298 Acc: 0.2147\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.137373 --> 2.129810).  Saving model ...\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 2.1067 Acc: 0.2994\n",
            "val Loss: 2.1818 Acc: 0.1580\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 2.0891 Acc: 0.3065\n",
            "val Loss: 2.1125 Acc: 0.1794\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.129810 --> 2.112520).  Saving model ...\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 2.0774 Acc: 0.3075\n",
            "val Loss: 2.1220 Acc: 0.1733\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 2.0722 Acc: 0.3126\n",
            "val Loss: 2.1754 Acc: 0.1656\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 2.0558 Acc: 0.3121\n",
            "val Loss: 2.0999 Acc: 0.2715\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.112520 --> 2.099854).  Saving model ...\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 2.0394 Acc: 0.3338\n",
            "val Loss: 2.0563 Acc: 0.2515\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.099854 --> 2.056343).  Saving model ...\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 2.0322 Acc: 0.3247\n",
            "val Loss: 2.0458 Acc: 0.2776\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.056343 --> 2.045803).  Saving model ...\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 2.0169 Acc: 0.3470\n",
            "val Loss: 2.0867 Acc: 0.1856\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 2.0126 Acc: 0.3338\n",
            "val Loss: 2.0652 Acc: 0.2331\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 2.0058 Acc: 0.3349\n",
            "val Loss: 1.9579 Acc: 0.3129\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (2.045803 --> 1.957852).  Saving model ...\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.9932 Acc: 0.3389\n",
            "val Loss: 2.0989 Acc: 0.1794\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.9789 Acc: 0.3607\n",
            "val Loss: 2.0086 Acc: 0.2853\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.9720 Acc: 0.3592\n",
            "val Loss: 1.9721 Acc: 0.2776\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 1.9625 Acc: 0.3708\n",
            "val Loss: 1.9877 Acc: 0.2807\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 1.9532 Acc: 0.3739\n",
            "val Loss: 1.9414 Acc: 0.3206\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.957852 --> 1.941449).  Saving model ...\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 1.9340 Acc: 0.3850\n",
            "val Loss: 1.9207 Acc: 0.3052\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.941449 --> 1.920702).  Saving model ...\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 1.9263 Acc: 0.3875\n",
            "val Loss: 1.9525 Acc: 0.3052\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 1.9215 Acc: 0.4007\n",
            "val Loss: 1.9486 Acc: 0.3206\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 1.9055 Acc: 0.3972\n",
            "val Loss: 1.8604 Acc: 0.4156\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.920702 --> 1.860372).  Saving model ...\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 1.9058 Acc: 0.3931\n",
            "val Loss: 1.8514 Acc: 0.4218\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.860372 --> 1.851412).  Saving model ...\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 1.8881 Acc: 0.4093\n",
            "val Loss: 1.8568 Acc: 0.3742\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 1.8824 Acc: 0.4098\n",
            "val Loss: 1.9116 Acc: 0.3236\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 1.8721 Acc: 0.4113\n",
            "val Loss: 1.8255 Acc: 0.4202\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.851412 --> 1.825470).  Saving model ...\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 1.8686 Acc: 0.4210\n",
            "val Loss: 1.8478 Acc: 0.3712\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 1.8474 Acc: 0.4179\n",
            "val Loss: 1.7974 Acc: 0.4417\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.825470 --> 1.797432).  Saving model ...\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 1.8439 Acc: 0.4281\n",
            "val Loss: 1.8661 Acc: 0.3696\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 1.8247 Acc: 0.4210\n",
            "val Loss: 1.8000 Acc: 0.4248\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 1.8350 Acc: 0.4134\n",
            "val Loss: 1.7368 Acc: 0.4847\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.797432 --> 1.736765).  Saving model ...\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 1.8033 Acc: 0.4422\n",
            "val Loss: 1.9259 Acc: 0.2822\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 1.8033 Acc: 0.4422\n",
            "val Loss: 1.7638 Acc: 0.3988\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 1.8020 Acc: 0.4311\n",
            "val Loss: 1.8195 Acc: 0.3543\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 1.7862 Acc: 0.4397\n",
            "val Loss: 1.7691 Acc: 0.4156\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 1.7680 Acc: 0.4585\n",
            "val Loss: 1.6799 Acc: 0.4985\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (1.736765 --> 1.679860).  Saving model ...\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 1.7637 Acc: 0.4610\n",
            "val Loss: 1.7642 Acc: 0.4064\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 1.7515 Acc: 0.4544\n",
            "val Loss: 1.7870 Acc: 0.3543\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 1.7488 Acc: 0.4615\n",
            "val Loss: 1.7063 Acc: 0.4463\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 1.7203 Acc: 0.4772\n",
            "val Loss: 1.7328 Acc: 0.4034\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 1.7292 Acc: 0.4848\n",
            "val Loss: 1.7037 Acc: 0.4770\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 1.7072 Acc: 0.4914\n",
            "val Loss: 1.7645 Acc: 0.4218\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 1.6953 Acc: 0.4802\n",
            "val Loss: 1.6477 Acc: 0.4801\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.679860 --> 1.647696).  Saving model ...\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 1.6533 Acc: 0.5162\n",
            "val Loss: 1.5676 Acc: 0.5521\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.647696 --> 1.567590).  Saving model ...\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 1.6254 Acc: 0.5304\n",
            "val Loss: 1.5669 Acc: 0.5506\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.567590 --> 1.566858).  Saving model ...\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 1.6170 Acc: 0.5395\n",
            "val Loss: 1.5281 Acc: 0.5997\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.566858 --> 1.528087).  Saving model ...\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 1.6124 Acc: 0.5223\n",
            "val Loss: 1.5369 Acc: 0.5767\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 1.6035 Acc: 0.5400\n",
            "val Loss: 1.5254 Acc: 0.5828\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.528087 --> 1.525362).  Saving model ...\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 1.6090 Acc: 0.5365\n",
            "val Loss: 1.5311 Acc: 0.5706\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 1.5986 Acc: 0.5481\n",
            "val Loss: 1.5158 Acc: 0.5874\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.525362 --> 1.515802).  Saving model ...\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 1.6017 Acc: 0.5481\n",
            "val Loss: 1.5013 Acc: 0.6058\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.515802 --> 1.501342).  Saving model ...\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 1.5923 Acc: 0.5608\n",
            "val Loss: 1.5223 Acc: 0.5813\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 1.5910 Acc: 0.5694\n",
            "val Loss: 1.5039 Acc: 0.5859\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 1.6039 Acc: 0.5400\n",
            "val Loss: 1.5129 Acc: 0.5905\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 1.5864 Acc: 0.5512\n",
            "val Loss: 1.4942 Acc: 0.6074\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.501342 --> 1.494187).  Saving model ...\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 1.5835 Acc: 0.5532\n",
            "val Loss: 1.4881 Acc: 0.6074\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.494187 --> 1.488143).  Saving model ...\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 1.5843 Acc: 0.5456\n",
            "val Loss: 1.5073 Acc: 0.6043\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 1.5739 Acc: 0.5628\n",
            "val Loss: 1.4955 Acc: 0.5920\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 1.5701 Acc: 0.5603\n",
            "val Loss: 1.4907 Acc: 0.5920\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 1.5677 Acc: 0.5567\n",
            "val Loss: 1.5119 Acc: 0.5859\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 1.5783 Acc: 0.5648\n",
            "val Loss: 1.4858 Acc: 0.5966\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.488143 --> 1.485839).  Saving model ...\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 1.5674 Acc: 0.5745\n",
            "val Loss: 1.4899 Acc: 0.6104\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 1.5738 Acc: 0.5638\n",
            "val Loss: 1.4823 Acc: 0.5966\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.485839 --> 1.482319).  Saving model ...\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 1.5699 Acc: 0.5628\n",
            "val Loss: 1.4793 Acc: 0.6181\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.482319 --> 1.479274).  Saving model ...\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 1.5673 Acc: 0.5714\n",
            "val Loss: 1.4530 Acc: 0.6196\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.479274 --> 1.452988).  Saving model ...\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 1.5587 Acc: 0.5628\n",
            "val Loss: 1.4739 Acc: 0.6135\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 1.5545 Acc: 0.5724\n",
            "val Loss: 1.4614 Acc: 0.6150\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 1.5452 Acc: 0.5770\n",
            "val Loss: 1.4872 Acc: 0.6028\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 1.5553 Acc: 0.5679\n",
            "val Loss: 1.5037 Acc: 0.5537\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 1.5572 Acc: 0.5669\n",
            "val Loss: 1.4491 Acc: 0.6120\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.452988 --> 1.449146).  Saving model ...\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 1.5450 Acc: 0.5795\n",
            "val Loss: 1.4960 Acc: 0.6074\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 1.5504 Acc: 0.5785\n",
            "val Loss: 1.4758 Acc: 0.6012\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 1.5425 Acc: 0.5805\n",
            "val Loss: 1.4440 Acc: 0.6258\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.449146 --> 1.443971).  Saving model ...\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 1.5435 Acc: 0.5831\n",
            "val Loss: 1.4334 Acc: 0.6135\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.443971 --> 1.433443).  Saving model ...\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 1.5417 Acc: 0.5785\n",
            "val Loss: 1.4589 Acc: 0.5997\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 1.5421 Acc: 0.5724\n",
            "val Loss: 1.4612 Acc: 0.6074\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 1.5421 Acc: 0.5775\n",
            "val Loss: 1.4497 Acc: 0.6120\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 1.5436 Acc: 0.5724\n",
            "val Loss: 1.4313 Acc: 0.6304\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.433443 --> 1.431252).  Saving model ...\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 1.5398 Acc: 0.5892\n",
            "val Loss: 1.4666 Acc: 0.6181\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 1.5290 Acc: 0.5892\n",
            "val Loss: 1.4800 Acc: 0.5844\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 1.5251 Acc: 0.5740\n",
            "val Loss: 1.4531 Acc: 0.6089\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 1.5375 Acc: 0.5811\n",
            "val Loss: 1.4140 Acc: 0.6595\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (1.431252 --> 1.414018).  Saving model ...\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 1.5240 Acc: 0.5968\n",
            "val Loss: 1.4301 Acc: 0.6166\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 1.5326 Acc: 0.5719\n",
            "val Loss: 1.4503 Acc: 0.6150\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 1.5248 Acc: 0.5871\n",
            "val Loss: 1.4437 Acc: 0.6396\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 1.5371 Acc: 0.5735\n",
            "val Loss: 1.4520 Acc: 0.6288\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 1.5235 Acc: 0.5745\n",
            "val Loss: 1.4566 Acc: 0.6104\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 1.5168 Acc: 0.5912\n",
            "val Loss: 1.4622 Acc: 0.6104\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 1.5144 Acc: 0.5917\n",
            "val Loss: 1.4317 Acc: 0.6089\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 1.5019 Acc: 0.6033\n",
            "val Loss: 1.4229 Acc: 0.6288\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 8 out of 11\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 1.4847 Acc: 0.6039\n",
            "val Loss: 1.4054 Acc: 0.6365\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (1.414018 --> 1.405385).  Saving model ...\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 1.4908 Acc: 0.6049\n",
            "val Loss: 1.4136 Acc: 0.6273\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 1.4913 Acc: 0.5998\n",
            "val Loss: 1.4019 Acc: 0.6365\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (1.405385 --> 1.401948).  Saving model ...\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 1.4994 Acc: 0.5993\n",
            "val Loss: 1.3991 Acc: 0.6488\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (1.401948 --> 1.399143).  Saving model ...\n",
            "Training complete in 15m 24s\n",
            "Best val Acc: 0.659509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA1cW-KHpREX"
      },
      "source": [
        "torch.save(model, './CustomCNN_model.pth')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzZ1zUHFxjcg"
      },
      "source": [
        "import natsort as nt\r\n",
        "from PIL import Image\r\n",
        "from torch.autograd import Variable \r\n",
        "def test_model():\r\n",
        "    data_transforms = transforms.Compose([\r\n",
        "        transforms.Grayscale(num_output_channels=1),\r\n",
        "        transforms.Resize([img_size,img_size]),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize([0.5], [0.5])\r\n",
        "    ])\r\n",
        "    model_ft = torch.load('./CustomCNN_model.pth', map_location=device)\r\n",
        "    \r\n",
        "    path_test = os.path.join(os.getcwd(), 'test')\r\n",
        "    image_list = nt.natsorted(os.listdir(path_test))\r\n",
        "    output_list = []\r\n",
        "    for i, images in enumerate(image_list):\r\n",
        "        path_test_image = os.path.join(path_test, images)\r\n",
        "        image = Image.open(path_test_image)\r\n",
        "        image = data_transforms(image)\r\n",
        "        image.unsqueeze_(dim=0)\r\n",
        "        image = Variable(image)\r\n",
        "        image = image.cuda(device)\r\n",
        "        torch.no_grad()\r\n",
        "        output = model(image)\r\n",
        "        #print(output)\r\n",
        "        output = torch.argmax(output, dim=1)\r\n",
        "        \r\n",
        "        output_list.append(output)\r\n",
        "        \r\n",
        "    return output_list\r\n",
        "output = test_model()\r\n",
        "\r\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DdmC8U_0IAy",
        "outputId": "0b0047e1-a131-4fe0-9cf7-5254efc32066"
      },
      "source": [
        "submission = pd.read_csv('./submission.csv')\r\n",
        "\r\n",
        "submission.digit = torch.cat(output).detach().cpu().numpy()\r\n",
        "print(submission)\r\n",
        "submission.to_csv('./CustomCNN_result.csv', index=False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id  digit\n",
            "0       2049      3\n",
            "1       2050      9\n",
            "2       2051      4\n",
            "3       2052      0\n",
            "4       2053      3\n",
            "...      ...    ...\n",
            "20475  22524      4\n",
            "20476  22525      1\n",
            "20477  22526      6\n",
            "20478  22527      1\n",
            "20479  22528      0\n",
            "\n",
            "[20480 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9y9m1100LxX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}